{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0gfAbD1BRax"
   },
   "source": [
    "# Assignment 5 #\n",
    "### Due: Friday, November 17th to be submitted via Canvas by 11:59 pm ###\n",
    "### Total points: **65** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Group 62**\n",
    "#### **UDIT DHAND (umd84)**\n",
    "#### **SANYAM JAIN (sj33448)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omOSpxMmBTQE"
   },
   "source": [
    "# Q1: Support Vector Machines (10 points)\n",
    "\n",
    "In this question, we will explore support vector machines for the Spam Base dataset from the UCI repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "0GbdHYxjLiok"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "tVj75ETFMaUJ"
   },
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "26ixmQ9qT6z3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ucimlrepo in /Users/uditdhand/anaconda3/lib/python3.10/site-packages (0.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('spambase.data', header=None)\n",
    "y = df[57]\n",
    "X = df.drop(columns=[57])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "DlQyfxg0YuB0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract: Classifying Email as Spam or Non-Spam\n",
      "Number of instances: 4601\n",
      "Number of features: 57\n",
      "0                 word_freq_make\n",
      "1              word_freq_address\n",
      "2                  word_freq_all\n",
      "3                   word_freq_3d\n",
      "4                  word_freq_our\n",
      "5                 word_freq_over\n",
      "6               word_freq_remove\n",
      "7             word_freq_internet\n",
      "8                word_freq_order\n",
      "9                 word_freq_mail\n",
      "10             word_freq_receive\n",
      "11                word_freq_will\n",
      "12              word_freq_people\n",
      "13              word_freq_report\n",
      "14           word_freq_addresses\n",
      "15                word_freq_free\n",
      "16            word_freq_business\n",
      "17               word_freq_email\n",
      "18                 word_freq_you\n",
      "19              word_freq_credit\n",
      "20                word_freq_your\n",
      "21                word_freq_font\n",
      "22                 word_freq_000\n",
      "23               word_freq_money\n",
      "24                  word_freq_hp\n",
      "25                 word_freq_hpl\n",
      "26              word_freq_george\n",
      "27                 word_freq_650\n",
      "28                 word_freq_lab\n",
      "29                word_freq_labs\n",
      "30              word_freq_telnet\n",
      "31                 word_freq_857\n",
      "32                word_freq_data\n",
      "33                 word_freq_415\n",
      "34                  word_freq_85\n",
      "35          word_freq_technology\n",
      "36                word_freq_1999\n",
      "37               word_freq_parts\n",
      "38                  word_freq_pm\n",
      "39              word_freq_direct\n",
      "40                  word_freq_cs\n",
      "41             word_freq_meeting\n",
      "42            word_freq_original\n",
      "43             word_freq_project\n",
      "44                  word_freq_re\n",
      "45                 word_freq_edu\n",
      "46               word_freq_table\n",
      "47          word_freq_conference\n",
      "48                   char_freq_;\n",
      "49                   char_freq_(\n",
      "50                   char_freq_[\n",
      "51                   char_freq_!\n",
      "52                   char_freq_$\n",
      "53                   char_freq_#\n",
      "54    capital_run_length_average\n",
      "55    capital_run_length_longest\n",
      "56      capital_run_length_total\n",
      "57                         Class\n",
      "Name: name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "# fetch dataset\n",
    "spambase = fetch_ucirepo(id=94)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "X = spambase.data.features\n",
    "y = spambase.data.targets\n",
    "print(\"Abstract:\", spambase.metadata['abstract'])\n",
    "print(\"Number of instances:\", spambase.metadata['num_instances'])\n",
    "print(\"Number of features:\", spambase.metadata['num_features'])\n",
    "print(spambase.variables['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "lmKGx3T8ZXci"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>word_freq_conference</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0            0.00               0.64           0.64           0.0   \n",
       "1            0.21               0.28           0.50           0.0   \n",
       "2            0.06               0.00           0.71           0.0   \n",
       "3            0.00               0.00           0.00           0.0   \n",
       "4            0.00               0.00           0.00           0.0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0           0.32            0.00              0.00                0.00   \n",
       "1           0.14            0.28              0.21                0.07   \n",
       "2           1.23            0.19              0.19                0.12   \n",
       "3           0.63            0.00              0.31                0.63   \n",
       "4           0.63            0.00              0.31                0.63   \n",
       "\n",
       "   word_freq_order  word_freq_mail  ...  word_freq_conference  char_freq_;  \\\n",
       "0             0.00            0.00  ...                   0.0         0.00   \n",
       "1             0.00            0.94  ...                   0.0         0.00   \n",
       "2             0.64            0.25  ...                   0.0         0.01   \n",
       "3             0.31            0.63  ...                   0.0         0.00   \n",
       "4             0.31            0.63  ...                   0.0         0.00   \n",
       "\n",
       "   char_freq_(  char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "0        0.000          0.0        0.778        0.000        0.000   \n",
       "1        0.132          0.0        0.372        0.180        0.048   \n",
       "2        0.143          0.0        0.276        0.184        0.010   \n",
       "3        0.137          0.0        0.137        0.000        0.000   \n",
       "4        0.135          0.0        0.135        0.000        0.000   \n",
       "\n",
       "   capital_run_length_average  capital_run_length_longest  \\\n",
       "0                       3.756                          61   \n",
       "1                       5.114                         101   \n",
       "2                       9.821                         485   \n",
       "3                       3.537                          40   \n",
       "4                       3.537                          40   \n",
       "\n",
       "   capital_run_length_total  \n",
       "0                       278  \n",
       "1                      1028  \n",
       "2                      2259  \n",
       "3                       191  \n",
       "4                       191  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "tukYZVbDZsG-"
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "y = y.to_numpy().squeeze()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=seed)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1jdODxjI41xm"
   },
   "source": [
    "a. (5 points) Implement the following function to train SVMs with a specified kernel type, hyper-parameter search space, and random state on the Spam Base dataset. Do hyper-parameter search over $C$ using [cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html), setting the number of folds to 5. After finding the best C, please use it to train the final model and return both the final model and the best C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "K90sxpVK7mED"
   },
   "outputs": [],
   "source": [
    "def search_best_svm(kernel, C_search_space, random_state):\n",
    "    best_score = -np.inf\n",
    "    for C in C_search_space:\n",
    "        # Initialize an SVM classifier with the specified kernel type, C value, and random state\n",
    "        ### START CODE ###\n",
    "        svm_mod = SVC(kernel=kernel, C=C, random_state=random_state)\n",
    "        ### END CODE ###\n",
    "\n",
    "\n",
    "        # Evaluate accuracy scores using 5-fold cross-validation scores on training set\n",
    "        ### START CODE ###\n",
    "        scores = cross_val_score(svm_mod, X_train, y_train, cv=5 , scoring=\"accuracy\")\n",
    "        ### END CODE ###\n",
    "\n",
    "        # Compute the average score and compare with the current best score to update the best C\n",
    "        ### START CODE ###\n",
    "        score = np.mean(scores)\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_C = C\n",
    "\n",
    "        ### END CODE ###\n",
    "        print(f\"C: {C} Avg Cross Val Score: {np.round(score, 4)}\")\n",
    "\n",
    "    print(f\"Best C: {best_C}\")\n",
    "\n",
    "    # Initialize the model using the specified kernel type, best C, and random state;\n",
    "    # and then fit the model using training set\n",
    "    ### START CODE ###\n",
    "    model = SVC(kernel=kernel, C=best_C, random_state=random_state)\n",
    "    model.fit(X_train, y_train)\n",
    "    ### END CODE ###\n",
    "    return model, best_C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZ_MU9Fl6EIn"
   },
   "source": [
    "b. (3 points) Run the function you implemented above to train SVMs with the search space of $C$ being [$0.1, 1, 10, 100$], random state set to 42, with the following three popular kernels: (i) linear (ii) polynomial (iii) RBF (Gaussian). Evaluate your final models on the test set and report their accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: linear\n",
      "C: 0.1 Avg Cross Val Score: 0.9214\n",
      "C: 1 Avg Cross Val Score: 0.9264\n",
      "C: 10 Avg Cross Val Score: 0.9283\n",
      "C: 100 Avg Cross Val Score: 0.9268\n",
      "Best C: 10\n",
      "Training Accuracy: 0.9355\n",
      "Test Accuracy: 0.9283\n",
      "===========================================\n",
      "Kernel: poly\n",
      "C: 0.1 Avg Cross Val Score: 0.6895\n",
      "C: 1 Avg Cross Val Score: 0.7594\n",
      "C: 10 Avg Cross Val Score: 0.8428\n",
      "C: 100 Avg Cross Val Score: 0.8957\n",
      "Best C: 100\n",
      "Training Accuracy: 0.9598\n",
      "Test Accuracy: 0.9125\n",
      "===========================================\n",
      "Kernel: rbf\n",
      "C: 0.1 Avg Cross Val Score: 0.896\n",
      "C: 1 Avg Cross Val Score: 0.9243\n",
      "C: 10 Avg Cross Val Score: 0.9243\n",
      "C: 100 Avg Cross Val Score: 0.9123\n",
      "Best C: 10\n",
      "Training Accuracy: 0.9681\n",
      "Test Accuracy: 0.9354\n",
      "===========================================\n",
      "===========================================\n",
      "Best Model: SVC(C=10, random_state=42)\n",
      "Best Kernel: rbf\n",
      "Best C: 10\n",
      "Best Test Accuracy: 0.935361216730038\n",
      "===========================================\n"
     ]
    }
   ],
   "source": [
    "search_space_c = [0.1,1,10,100]\n",
    "kernel_type = ['linear', 'poly', 'rbf']\n",
    "random_state = 42\n",
    "\n",
    "\n",
    "model_accuracy_dict = {'linear': -np.inf, 'poly': -np.inf, 'rbf': -np.inf}\n",
    "\n",
    "best_model = None\n",
    "best_test_acc = -np.inf\n",
    "best_kernel = None\n",
    "\n",
    "for kernel in kernel_type:\n",
    "    print(f\"Kernel: {kernel}\")\n",
    "    model, best_C = search_best_svm(kernel, search_space_c, random_state)\n",
    "    print(f\"Training Accuracy: {np.round(model.score(X_train, y_train), 4)}\")\n",
    "    print(f\"Test Accuracy: {np.round(model.score(X_test, y_test), 4)}\")\n",
    "    print(\"===========================================\")\n",
    "\n",
    "\n",
    "    if model.score(X_test, y_test) > best_test_acc:\n",
    "        best_test_acc = model.score(X_test, y_test)\n",
    "        best_model = model\n",
    "        best_kernel = kernel\n",
    "    \n",
    "    model_accuracy_dict[kernel] = model.score(X_test, y_test)\n",
    "\n",
    "print(f\"Best Model: {best_model}\")\n",
    "print(f\"Best Kernel: {best_kernel}\")\n",
    "print(f\"Best C: {best_C}\")\n",
    "print(f\"Best Test Accuracy: {best_test_acc}\")\n",
    "print(\"===========================================\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCxwqenL6nZ4"
   },
   "source": [
    "c. (2 points) Train a logistic regression model using the training set. Compare its performance with that of the SVMs trained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model Test set Accuracy:  0.9223\n",
      "SVM linear Test Set Accuracy: 0.9283\n",
      "SVM poly Test Set Accuracy: 0.9125\n",
      "SVM rbf Test Set Accuracy: 0.9354\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Initialize a logistic regression classifier\n",
    "log_reg = LogisticRegression(random_state=seed, max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred_log = log_reg.predict(X_test)\n",
    "acc_log = accuracy_score(y_test, y_pred_log)\n",
    "\n",
    "\n",
    "print(\"Logistic Regression Model Test set Accuracy: \", round(acc_log, 4))\n",
    "\n",
    "for key ,value in model_accuracy_dict.items():\n",
    "    print(f\"SVM {key} Test Set Accuracy: {round(value, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the test set accuracy of logistic regression model is very similar to that of SVM models. However, SVM model with RBF kernel is slightly better than Logistic Regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bc9ca1b5"
   },
   "source": [
    "# Question 2 : Ensemble Methods for Classification (25 pts)\n",
    "\n",
    "In this question, we will compare the performances of different ensemble methods for classification problems: [Bagging](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html), [AdaBoost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html), [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) and [XGBoost](https://xgboost.readthedocs.io/en/stable/python/python_api.html) classifiers.\n",
    "\n",
    "We will look at the [GiveMeSomeCredit](https://www.kaggle.com/c/GiveMeSomeCredit) dataset for this question. The dataset is extremely large so for this question we will only consider a subset which has been provided along with the notebook for this assignment. The dataset has already been split into train and test sets.\n",
    "\n",
    "The task is to predict the probability that someone will experience financial distress in the next two years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "executionInfo": {
     "elapsed": 6176,
     "status": "ok",
     "timestamp": 1699650398287,
     "user": {
      "displayName": "Song Wang",
      "userId": "14539824110514115974"
     },
     "user_tz": 360
    },
    "id": "a93b913f",
    "outputId": "789864e1-5a42-4df6-a7cd-4cea1102cde2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-dc578b7f-3ab1-4613-95e2-300439976811\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-dc578b7f-3ab1-4613-95e2-300439976811\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving hw5_data.csv to hw5_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Only use this code block if you are using Google Colab.\n",
    "# If you are using Jupyter Notebook, please ignore this code block. You can directly upload the file to your Jupyter Notebook file systems.\n",
    "from google.colab import files\n",
    "\n",
    "## It will prompt you to select a local file. Click on “Choose Files” then select and upload the file.\n",
    "## Wait for the file to be 100% uploaded. You should see the name of the file once Colab has uploaded it.\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "1a2bac93"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SeriousDlqin2yrs</th>\n",
       "      <th>RevolvingUtilizationOfUnsecuredLines</th>\n",
       "      <th>age</th>\n",
       "      <th>NumberOfTime30-59DaysPastDueNotWorse</th>\n",
       "      <th>DebtRatio</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "      <th>NumberOfOpenCreditLinesAndLoans</th>\n",
       "      <th>NumberOfTimes90DaysLate</th>\n",
       "      <th>NumberRealEstateLoansOrLines</th>\n",
       "      <th>NumberOfTime60-89DaysPastDueNotWorse</th>\n",
       "      <th>NumberOfDependents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.571373</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>0.430620</td>\n",
       "      <td>9274.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.233999</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0.257380</td>\n",
       "      <td>5656.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.299270</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0.114575</td>\n",
       "      <td>4747.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.032165</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0.308326</td>\n",
       "      <td>8490.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.050591</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0.862627</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SeriousDlqin2yrs  RevolvingUtilizationOfUnsecuredLines  age  \\\n",
       "0                 0                              0.571373   66   \n",
       "1                 0                              0.233999   56   \n",
       "2                 0                              0.299270   33   \n",
       "3                 0                              0.032165   41   \n",
       "4                 0                              0.050591   36   \n",
       "\n",
       "   NumberOfTime30-59DaysPastDueNotWorse  DebtRatio  MonthlyIncome  \\\n",
       "0                                     0   0.430620         9274.0   \n",
       "1                                     0   0.257380         5656.0   \n",
       "2                                     0   0.114575         4747.0   \n",
       "3                                     0   0.308326         8490.0   \n",
       "4                                     0   0.862627         3333.0   \n",
       "\n",
       "   NumberOfOpenCreditLinesAndLoans  NumberOfTimes90DaysLate  \\\n",
       "0                               10                        0   \n",
       "1                               12                        0   \n",
       "2                                8                        0   \n",
       "3                                8                        0   \n",
       "4                                8                        0   \n",
       "\n",
       "   NumberRealEstateLoansOrLines  NumberOfTime60-89DaysPastDueNotWorse  \\\n",
       "0                             1                                     0   \n",
       "1                             0                                     0   \n",
       "2                             0                                     0   \n",
       "3                             1                                     0   \n",
       "4                             2                                     0   \n",
       "\n",
       "   NumberOfDependents  \n",
       "0                 0.0  \n",
       "1                 0.0  \n",
       "2                 3.0  \n",
       "3                 0.0  \n",
       "4                 0.0  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('hw5_data.csv')\n",
    "data.drop(data.columns[data.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "fb34060d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (3750, 10) (3750,)\n",
      "test: (1250, 10) (1250,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = data['SeriousDlqin2yrs']\n",
    "X = data.drop(['SeriousDlqin2yrs'],axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 7)\n",
    "\n",
    "print('train:',X_train.shape, y_train.shape)\n",
    "print('test:',X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import (train_test_split,GridSearchCV)\n",
    "from sklearn.metrics import (accuracy_score,roc_auc_score)\n",
    "from sklearn.ensemble import (RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier)\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from time import time\n",
    "import xgboost\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "420aeb16"
   },
   "outputs": [],
   "source": [
    "columns_list = list(X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f40f9107"
   },
   "source": [
    "a. (2.5 pts) Fit a Decision Tree Classifier with random_state = 14 for this classification problem. Report the accuracy_score and roc_auc_score on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "45de59e6"
   },
   "outputs": [],
   "source": [
    "def fit_classifier(clf):\n",
    "  # Fit the classifier on the training set\n",
    "  ### START CODE ###\n",
    "  clf.fit(X_train, y_train)\n",
    "  ### END CODE ###\n",
    "  return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "yZfiYRKtHJQ4"
   },
   "outputs": [],
   "source": [
    "def evaluate_classifier(clf, X_test, y_test):\n",
    "  # Compute the accuracy_score, and roc_auc_score on the test set\n",
    "  ### START CODE ###\n",
    "  y_pred = clf.predict(X_test)\n",
    "  y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "  acc_score = accuracy_score(y_test, y_pred)\n",
    "  auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "  ### END CODE ###\n",
    "  print(\"Accuracy_score: {}, ROC_AUC_score: {}\".format(acc_score, auc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "9a436542"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Accuracy_score: 0.888, ROC_AUC_score: 0.5854582176218127\n"
     ]
    }
   ],
   "source": [
    "print(\"Decision Tree\")\n",
    "# Initialize your decision tree classifier\n",
    "### START CODE ###\n",
    "dt_clf = DecisionTreeClassifier(random_state=14)\n",
    "### END CODE ###\n",
    "\n",
    "dt_clf = fit_classifier(dt_clf)\n",
    "evaluate_classifier(dt_clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6d360a99"
   },
   "source": [
    "b. (2.5 pts) Create a [Bagging](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) of 25 classifiers (i.e, n_estimators=25) with random_state=14. Please use Decision Tree Classifier with random_state=14 as the base classifier. Report accuracy_score and roc_auc_score on the test data for this emsemble classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "239c9c3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging of Decesion Trees\n",
      "Accuracy_score: 0.9272, ROC_AUC_score: 0.7866043928300934\n"
     ]
    }
   ],
   "source": [
    "print(\"Bagging of Decesion Trees\")\n",
    "# Initialize your bagging classifier\n",
    "### START CODE ###\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(random_state=14), n_estimators=25, random_state=14)\n",
    "### END CODE ###\n",
    "\n",
    "bag_clf = fit_classifier(bag_clf)\n",
    "evaluate_classifier(bag_clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34225ee3"
   },
   "source": [
    "c. (5 pts) In this question, you will fit a [Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) model on the training data for this classification task.\n",
    "\n",
    "1. First, please find the best parameters (including *n_estimators*, *max_features* and *criterion*) using [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). Report the optimal parameters obtained by GridSearch.\n",
    "2. Fit a model using the best parameters, and report the [confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) and [roc_auc_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score) on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "R9E7sMGt2goM"
   },
   "outputs": [],
   "source": [
    "def grid_search_for_classifier(clf, param_grid, X_train, y_train):\n",
    "  # Grid search\n",
    "  grid_search = GridSearchCV(clf, param_grid=param_grid)\n",
    "\n",
    "  # Conduct grid search using the training set (1 line of code only)\n",
    "  ### START CODE ###\n",
    "  grid_search.fit(X_train, y_train)\n",
    "  ### END CODE ###\n",
    "  print(grid_search.best_params_)\n",
    "\n",
    "  # Set the best paramters for your clf (1 line of code only)\n",
    "  ### START CODE ###\n",
    "  clf.set_params(**grid_search.best_params_)\n",
    "  ### END CODE ###\n",
    "  return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "4UyA2_rB2gG9"
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate_classifier(clf, X_train, y_train, X_test, y_test):\n",
    "  t0 = time()\n",
    "  # Fit your classifier on the training set\n",
    "  ### START CODE ###\n",
    "  clf.fit(X_train, y_train)\n",
    "  ### END CODE ###\n",
    "  print(\"training time\", round(time()-t0, 3), \"s\")\n",
    "\n",
    "  t0 = time()\n",
    "  y_pred = clf.predict(X_test)\n",
    "  print(\"predict time\", round(time()-t0, 3), \"s\")\n",
    "\n",
    "  print(\"Confusion matrix: \")\n",
    "  # Print the confusion matrix computed from the test set (1 line of code only)\n",
    "  ### START CODE ###\n",
    "  print(confusion_matrix(y_test, y_pred))\n",
    "  ### END CODE ###\n",
    "\n",
    "\n",
    "  ### START CODE ###\n",
    "  y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
    "  acc_score = accuracy_score(y_test, y_pred)\n",
    "  auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "  ### END CODE ###\n",
    "\n",
    "  print(\"Accuracy: {}, AUC_ROC: {}\".format(acc_score, auc_score))\n",
    "  return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "6bfb9542"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'max_features': 1, 'n_estimators': 100, 'random_state': 17}\n",
      "training time 0.292 s\n",
      "predict time 0.019 s\n",
      "Confusion matrix: \n",
      "[[1160    5]\n",
      " [  81    4]]\n",
      "Accuracy: 0.9312, AUC_ROC: 0.8378591264832114\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(criterion=&#x27;entropy&#x27;, max_features=1, random_state=17)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(criterion=&#x27;entropy&#x27;, max_features=1, random_state=17)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', max_features=1, random_state=17)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\"n_estimators\": [1, 10, 50, 100],\n",
    "              \"max_features\": [1, 5, 10, \"auto\"],\n",
    "              \"criterion\": ['gini','entropy'],\n",
    "              \"random_state\": [17]}\n",
    "\n",
    "# Initialize your random forest classifier\n",
    "### START CODE ###\n",
    "rf_clf = RandomForestClassifier()\n",
    "### END CODE ###\n",
    "rf_clf = grid_search_for_classifier(rf_clf, param_grid, X_train, y_train)\n",
    "train_and_evaluate_classifier(rf_clf, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "314c313b"
   },
   "source": [
    "d. (10 pts) This time, let us use [AdaBoost](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier) and [XGBoost](https://xgboost.readthedocs.io/en/stable/python/python_api.html) for the same task. For AdaBoost and XGBoost, please respectively find the best parameters (including *n_estimators, learning_rate*); fit your model using the best parameters, and report the confusion matrix and roc_auc_score on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "3222ef25"
   },
   "outputs": [],
   "source": [
    "param_grid = {\"n_estimators\": [10, 100],\n",
    "          \"learning_rate\": [0.01, 0.1, 0.5],\n",
    "          \"random_state\": [17]\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "bp4Ekjpy1cGw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.1, 'n_estimators': 100, 'random_state': 17}\n",
      "training time 0.311 s\n",
      "predict time 0.015 s\n",
      "Confusion matrix: \n",
      "[[1153   12]\n",
      " [  72   13]]\n",
      "Accuracy: 0.9328, AUC_ROC: 0.8390254986114618\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AdaBoostClassifier(learning_rate=0.1, n_estimators=100, random_state=17)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AdaBoostClassifier</label><div class=\"sk-toggleable__content\"><pre>AdaBoostClassifier(learning_rate=0.1, n_estimators=100, random_state=17)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "AdaBoostClassifier(learning_rate=0.1, n_estimators=100, random_state=17)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize your AdaBoost classifier\n",
    "### START CODE ###\n",
    "ab_clf = AdaBoostClassifier()\n",
    "### END CODE ###\n",
    "ab_clf = grid_search_for_classifier(ab_clf, param_grid, X_train, y_train)\n",
    "train_and_evaluate_classifier(ab_clf, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "Hv0p1mZh2CqA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.01, 'n_estimators': 100, 'random_state': 17}\n",
      "training time 0.227 s\n",
      "predict time 0.004 s\n",
      "Confusion matrix: \n",
      "[[1149   16]\n",
      " [  78    7]]\n",
      "Accuracy: 0.9248, AUC_ROC: 0.814915425397627\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=17, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=17, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=17, ...)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize your XGBoost classifier\n",
    "### START CODE ###\n",
    "xgb_clf = xgboost.XGBClassifier()\n",
    "### END CODE ###\n",
    "xgb_clf = grid_search_for_classifier(xgb_clf, param_grid, X_train, y_train)\n",
    "train_and_evaluate_classifier(xgb_clf, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6bc87cd"
   },
   "source": [
    "f. (5 pts) Compare the performance of decision tree from part a) with the ensemble methods. Briefly explain which of the three ensemble methods performed better and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compared to decision tree**: The ensemble methods (Bagging, RandomForest, AdaBoost) were all better than just using one decision tree. They all got higher scores for accuracy and ROC_AUC.\n",
    "\n",
    "**Reason**: These methods work well because they use more than one model to make a guess. This usually makes them more right and less likely to make mistakes than just one model. In other words, they reduce both bias and variance.\n",
    "\n",
    "**Comparing the Ensembe Methods**:\n",
    "\n",
    "- AdaBoost performs best: It's a little more right than the others and has better ROC_AUC scores. It works by paying more attention to where it went wrong before and trying to fix those mistakes.\n",
    "\n",
    "- RandomForest is almost as good: It's very close to AdaBoost and does a good job too. It makes a lot of decision trees and picks the answer that most trees agree on. It's really good for different kinds of data but might not be as sharp as AdaBoost.\n",
    "\n",
    "- Bagging is better than DT but not the best: It's much better than just one tree because it makes less mistakes. But it doesn't do as good a job as AdaBoost or RandomForest because it doesn't focus on the parts where it's wrong.\n",
    "\n",
    "**Conclusion**: AdaBoost did the best job with this data because it really focuses on what it got wrong before and keeps getting better each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZXTR1tM7yBp"
   },
   "source": [
    "# Q3: CatBoost (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zcExque_yso"
   },
   "source": [
    "In this question you will learn about a boosting algorithm known as **CatBoost**. Please go through the two videos specified below to get a better understanding of the CatBoost algorithm and answer the questions that follow.\n",
    "\n",
    "[Part-1](https://www.youtube.com/watch?v=KXOTSkPL2X4&ab_channel=StatQuestwithJoshStarmer)\n",
    "[Part - 2](https://www.youtube.com/watch?v=3Bg2XRFOTzg&t=242s&ab_channel=StatQuestwithJoshStarmer)\n",
    "\n",
    "\n",
    "\n",
    "a. **(5 points)** Briefly explain Ordered Target Encoding. What challenge does it try to address?\n",
    "\n",
    "b. **(5 points)** Briefly describe the main advantages and disadvantages of CatBoost as compared to XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer a.\n",
    "\n",
    "**Ordered Target Encoding** turns categories into numbers for machine learning. It's good for when you have colors or other categories that don't have numbers. \n",
    "\n",
    "Instead of making lots of new columns for each category, or just giving each one a random number, Ordered Target Encoding gives each category a number that means something important. Like if you're looking at house prices, it would give each neighborhood a number that's the average price of houses there.\n",
    "\n",
    "**Using Ordered Target Encoding:**\n",
    "\n",
    "Ordered Target Encoding deals with some big problems when turning categories into numbers for machine learning:\n",
    "\n",
    "- It keeps all the important stuff and works well when there are lots of categories (high cardinality). Other ways like One-Hot Encoding can make too many columns if there are many categories, which can slow things down and make models too specific (overfit). Label Encoding can give categories random numbers that don't really match up with what you're trying to predict. But Ordered Target Encoding keeps the categories simple and still shows how they're related to what you're trying to predict. It uses an important number from what you're trying to predict (like the average price) for each category.\n",
    "\n",
    "- It shows how categories affect what you're trying to predict. It uses the target thing itself to make the numbers, so it really captures how each category impacts the outcome.\n",
    "\n",
    "So, Ordered Target Encoding changes categories into numbers in a way that keeps their link to what you're trying to predict. It helps keep things simple, stops the model from being too specific, and keeps the important links between the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer b.\n",
    "\n",
    "Comparing CatBoost and XGBoost:\n",
    "\n",
    "1. **Categorical Data:**\n",
    "   - **CatBoost:** Great at working with categories without needing extra steps.\n",
    "   - **XGBoost:** You have to get categories ready first, which takes extra work.<br><br>\n",
    "\n",
    "2. **Speed and Performance:**\n",
    "   - **CatBoost:** Works well with categories but might be slower with just numbers.\n",
    "   - **XGBoost:** Usually trains faster, especially with numbers.<br><br>\n",
    "\n",
    "3. **Overfitting:**\n",
    "   - **CatBoost:** Better at not fitting too much, good for small data.\n",
    "   - **XGBoost:** Can fit too much if you're not careful with the settings.<br><br>\n",
    "\n",
    "4. **Help and Learning Resources:**\n",
    "   - **CatBoost:** Has some support but not as much as XGBoost.\n",
    "   - **XGBoost:** Lots of help and guides out there.<br><br>\n",
    "\n",
    "5. **Parameter Tuning:**\n",
    "   - **CatBoost:** Easier to start with but not as many tweaks you can make.\n",
    "   - **XGBoost:** Lots of tweaks possible, but you need to know what you're doing.<br><br>\n",
    "\n",
    "In short, use CatBoost if you have categories in your data. Pick XGBoost for number data or when you want to make a lot of specific adjustments to your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTFRT23o76A2"
   },
   "source": [
    "# Q4: Convolutional Neural Network (20 points)\n",
    "In this question, we will continue our exercise on the SVHN classification task from the previous homework, but this time we will be using Convolutional Neural Networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "lW0odgzmXUU_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "Dr0KdJ2LrCCV"
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "oBCdEj-_0tkS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://ufldl.stanford.edu/housenumbers/train_32x32.mat to ./train_32x32.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182040794/182040794 [00:23<00:00, 7879576.26it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://ufldl.stanford.edu/housenumbers/test_32x32.mat to ./test_32x32.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64275384/64275384 [00:20<00:00, 3062800.40it/s]\n"
     ]
    }
   ],
   "source": [
    "transform=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.4376821, 0.4437697, 0.47280442), (0.19803012, 0.20101562, 0.19703614))\n",
    "        ])\n",
    "\n",
    "train_dataset = torchvision.datasets.SVHN(root='.', split='train', transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.SVHN(root='.', split='test', transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "iLn4YKK90vjo"
   },
   "outputs": [],
   "source": [
    "train_num = int(len(train_dataset) * 0.8)\n",
    "val_num = len(train_dataset) - train_num\n",
    "# Randomly split the training dataset into training dataset and validation dataset\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_num, val_num])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFUXsbsqjoCw"
   },
   "source": [
    "a. (10 points) Build a convolutional neural network with the following sequential configuration. If not specified, please use the default setting of torch.nn.Conv2d. The output of the convolution layers will be fed into a fully-connected MLP. Then train the model with Adam optimizer (lr=1e-3) for 10 epochs. You should be able to achieve test accuracy of over 85%.\n",
    "\n",
    "\n",
    "\n",
    "> Layer 1\n",
    "*   2d convolution (# input channel=3, # output channel=16, kernel size=3, padding=1)\n",
    "*   2d batch normalization\n",
    "*   Relu activation\n",
    "\n",
    "> Pool 1\n",
    "*   2d max pooling (kernel size=2)\n",
    "\n",
    "> Layer 2\n",
    "*   2d convolution (# output channel=16, kernel size=3, padding=1)\n",
    "*   2d batch normalization\n",
    "*   Relu activation\n",
    "\n",
    "> Pool 2\n",
    "*   2d max pooling (kernel size=2)\n",
    "\n",
    "> Layer 3\n",
    "*   2d convolution (# output channel=32, kernel size=3, padding=1)\n",
    "*   2d batch normalization\n",
    "*   Relu activation\n",
    "\n",
    "> Pool 3\n",
    "*   2d max pooling (kernel size=2)\n",
    "\n",
    "References:\n",
    "\n",
    "*   https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "*   https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html\n",
    "*   https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html\n",
    "*   https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "M8OcO88V01Rr"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, pool=True):\n",
    "        super(CNN, self).__init__()\n",
    "        self.pool = pool\n",
    "\n",
    "        # Create convolutional layers\n",
    "        ### START CODE ###\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.pool1 =  nn.MaxPool2d(kernel_size=2) if self.pool else None\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2) if self.pool else None\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2) if self.pool else None\n",
    "        ### END CODE ###\n",
    "\n",
    "        # Create fully connected layers (nn.Linear)\n",
    "        if self.pool:\n",
    "            self.mlp1 = nn.Linear(32*4*4, 50)\n",
    "        else:\n",
    "            self.mlp1 = nn.Linear(32*32*32, 50)\n",
    "\n",
    "        self.mlp2 = nn.Linear(50, 50)\n",
    "        self.mlp3 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        if self.pool:\n",
    "            x = self.pool1(x)\n",
    "\n",
    "        x = self.layer2(x)\n",
    "        if self.pool:\n",
    "            x = self.pool2(x)\n",
    "\n",
    "        x = self.layer3(x)\n",
    "        if self.pool:\n",
    "            x = self.pool3(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "\n",
    "        x = F.relu(self.mlp1(x))\n",
    "        x = F.relu(self.mlp2(x))\n",
    "        x = self.mlp3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "Ygrxekov097w"
   },
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_num = 0\n",
    "    for data, target in tqdm(loader):\n",
    "        out = model(data)\n",
    "        # Calculate loss based on model output and target\n",
    "        loss = F.nll_loss(F.log_softmax(out, dim=1), target)\n",
    "\n",
    "        # Use the optimizer to perform backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_size = len(target)\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_num += batch_size\n",
    "    avg_loss = total_loss / total_num\n",
    "    return avg_loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_num = 0\n",
    "    for data, target in tqdm(loader):\n",
    "        out = model(data)\n",
    "        # Calculate loss based on model output and target\n",
    "        loss = F.nll_loss(F.log_softmax(out, dim=1), target)\n",
    "\n",
    "        # Get model's prediction\n",
    "        pred = torch.argmax(out, dim=1)\n",
    "\n",
    "        # Count number of correct predictions\n",
    "        correct = accuracy_score(target, pred, normalize=False)\n",
    "\n",
    "        total_correct += correct\n",
    "        batch_size = len(target)\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_num += batch_size\n",
    "    avg_loss = total_loss / total_num\n",
    "    acc = total_correct / total_num\n",
    "    return avg_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "aET_5IqXDsC1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229/229 [00:37<00:00,  6.03it/s]\n",
      "100%|██████████| 58/58 [00:03<00:00, 15.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train Loss: 1.2216163216307652 Val Loss: 0.6390311451124878 Val Acc: 0.8097188097188097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229/229 [00:35<00:00,  6.37it/s]\n",
      "100%|██████████| 58/58 [00:04<00:00, 14.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Train Loss: 0.5604904653065726 Val Loss: 0.49842555526010396 Val Acc: 0.855036855036855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229/229 [00:35<00:00,  6.43it/s]\n",
      "100%|██████████| 58/58 [00:04<00:00, 13.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Train Loss: 0.46001797251841303 Val Loss: 0.4715866463173943 Val Acc: 0.8568796068796068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229/229 [00:35<00:00,  6.47it/s]\n",
      "100%|██████████| 58/58 [00:03<00:00, 16.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Train Loss: 0.4117995898391635 Val Loss: 0.42648188253178676 Val Acc: 0.8725771225771226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229/229 [00:34<00:00,  6.71it/s]\n",
      "100%|██████████| 58/58 [00:03<00:00, 16.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Train Loss: 0.3775536270436955 Val Loss: 0.4109084661455061 Val Acc: 0.8780371280371281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229/229 [00:33<00:00,  6.75it/s]\n",
      "100%|██████████| 58/58 [00:03<00:00, 16.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Train Loss: 0.3546164079075451 Val Loss: 0.4086883616575998 Val Acc: 0.8751023751023751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229/229 [00:34<00:00,  6.73it/s]\n",
      "100%|██████████| 58/58 [00:03<00:00, 16.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Train Loss: 0.3333229528643269 Val Loss: 0.38742386854609406 Val Acc: 0.8827463827463827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229/229 [00:34<00:00,  6.72it/s]\n",
      "100%|██████████| 58/58 [00:03<00:00, 16.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Train Loss: 0.3197165992828311 Val Loss: 0.3769732241971617 Val Acc: 0.8881381381381381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229/229 [00:34<00:00,  6.60it/s]\n",
      "100%|██████████| 58/58 [00:03<00:00, 16.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Train Loss: 0.3017301185099951 Val Loss: 0.3783684137241456 Val Acc: 0.8873191373191374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229/229 [00:33<00:00,  6.75it/s]\n",
      "100%|██████████| 58/58 [00:03<00:00, 16.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Train Loss: 0.2924189313306671 Val Loss: 0.37169599677089 Val Acc: 0.8915506415506416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model1 = CNN(pool=True)\n",
    "optimizer = torch.optim.Adam(model1.parameters(), lr=1e-3)\n",
    "best_acc = -np.inf\n",
    "epochs = 10\n",
    "for e in range(1, epochs + 1):\n",
    "    train_loss = train(model1, train_loader, optimizer)\n",
    "    val_loss, val_acc = eval(model1, val_loader)\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_model1 = model1\n",
    "    print(f\"Epoch: {e} Train Loss: {train_loss} Val Loss: {val_loss} Val Acc: {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "A7Za_DdlDwJX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102/102 [00:06<00:00, 16.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_, test_acc = eval(best_model1, test_loader)\n",
    "print(f\"Test accuracy: {np.round(test_acc, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLAnNsjsnRpK"
   },
   "source": [
    "b. (5 points) Use torch-summary to print a summary of the model. The number of parameters should be less than the one of the MLP we trained in the previous homework. Why does it have less number of parameters but have higher accuracy?\n",
    "\n",
    "Reference\n",
    "*   https://pypi.org/project/torch-summary/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "tkuRsoNxR8dX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-summary\n",
      "  Downloading torch_summary-1.4.5-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: torch-summary\n",
      "Successfully installed torch-summary-1.4.5\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Sequential: 1-1                        --\n",
      "|    └─Conv2d: 2-1                       448\n",
      "|    └─BatchNorm2d: 2-2                  32\n",
      "|    └─ReLU: 2-3                         --\n",
      "├─MaxPool2d: 1-2                         --\n",
      "├─Sequential: 1-3                        --\n",
      "|    └─Conv2d: 2-4                       2,320\n",
      "|    └─BatchNorm2d: 2-5                  32\n",
      "|    └─ReLU: 2-6                         --\n",
      "├─MaxPool2d: 1-4                         --\n",
      "├─Sequential: 1-5                        --\n",
      "|    └─Conv2d: 2-7                       4,640\n",
      "|    └─BatchNorm2d: 2-8                  64\n",
      "|    └─ReLU: 2-9                         --\n",
      "├─MaxPool2d: 1-6                         --\n",
      "├─Linear: 1-7                            25,650\n",
      "├─Linear: 1-8                            2,550\n",
      "├─Linear: 1-9                            510\n",
      "=================================================================\n",
      "Total params: 36,246\n",
      "Trainable params: 36,246\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─Sequential: 1-1                        --\n",
       "|    └─Conv2d: 2-1                       448\n",
       "|    └─BatchNorm2d: 2-2                  32\n",
       "|    └─ReLU: 2-3                         --\n",
       "├─MaxPool2d: 1-2                         --\n",
       "├─Sequential: 1-3                        --\n",
       "|    └─Conv2d: 2-4                       2,320\n",
       "|    └─BatchNorm2d: 2-5                  32\n",
       "|    └─ReLU: 2-6                         --\n",
       "├─MaxPool2d: 1-4                         --\n",
       "├─Sequential: 1-5                        --\n",
       "|    └─Conv2d: 2-7                       4,640\n",
       "|    └─BatchNorm2d: 2-8                  64\n",
       "|    └─ReLU: 2-9                         --\n",
       "├─MaxPool2d: 1-6                         --\n",
       "├─Linear: 1-7                            25,650\n",
       "├─Linear: 1-8                            2,550\n",
       "├─Linear: 1-9                            510\n",
       "=================================================================\n",
       "Total params: 36,246\n",
       "Trainable params: 36,246\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model1, input_size=(3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNNs have fewer parameters and get better results than MLPs for a couple of reasons:\n",
    "\n",
    "- **They use parameters better**: CNNs share weights and only connect some neurons together, which means they need fewer parameters. This helps them learn without overfitting.\n",
    "\n",
    "- **They're good at picking out important parts of images**: CNNs can automatically find important patterns in images, like edges and textures. This is something MLPs can't do, and it's why CNNs are really good for working with images.\n",
    "\n",
    "- **They can handle new data well**: CNNs are built in a way that helps them work well with data they haven't seen before. That's key for doing well on tasks like telling what's in an image.\n",
    "\n",
    "Conclusion: CNNs need fewer parameters because of how they share weights and connect neurons, and they're more accurate at image tasks because they're really good at learning from the patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8eb6rNHsobVb"
   },
   "source": [
    "c. (5 points) Train another CNN with the pool option set to False. What are the differences in terms of accuracy or computation caused by disabling max pooling? What are the effects of pooling operations in CNNs? (This might take some time. Watch a TV show while you're waiting for the results..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "1m-9yG9VAbgE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229/229 [01:38<00:00,  2.32it/s]\n",
      "100%|██████████| 58/58 [00:07<00:00,  8.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train Loss: 1.1319699598674775 Val Loss: 0.5539264180297054 Val Acc: 0.834971334971335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229/229 [01:40<00:00,  2.27it/s]\n",
      "100%|██████████| 58/58 [00:06<00:00,  8.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Train Loss: 0.5003900743732984 Val Loss: 0.5062575723071779 Val Acc: 0.8523068523068523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229/229 [01:39<00:00,  2.29it/s]\n",
      "100%|██████████| 58/58 [00:08<00:00,  6.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Train Loss: 0.418453345512044 Val Loss: 0.4466331599190233 Val Acc: 0.8661616161616161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229/229 [01:40<00:00,  2.28it/s]\n",
      "100%|██████████| 58/58 [00:07<00:00,  7.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Train Loss: 0.3647330720248091 Val Loss: 0.43529063948119173 Val Acc: 0.8748293748293748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229/229 [01:45<00:00,  2.18it/s]\n",
      "100%|██████████| 58/58 [00:06<00:00,  8.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Train Loss: 0.32424842470406817 Val Loss: 0.425377467576513 Val Acc: 0.875989625989626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229/229 [01:35<00:00,  2.39it/s]\n",
      "100%|██████████| 58/58 [00:06<00:00,  8.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Train Loss: 0.29184436213592857 Val Loss: 0.4364793598285764 Val Acc: 0.8747611247611248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229/229 [01:37<00:00,  2.35it/s]\n",
      "100%|██████████| 58/58 [00:07<00:00,  7.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Train Loss: 0.2637728871730456 Val Loss: 0.4232715476724316 Val Acc: 0.8806988806988807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229/229 [01:45<00:00,  2.16it/s]\n",
      "100%|██████████| 58/58 [00:07<00:00,  7.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Train Loss: 0.23433508727750052 Val Loss: 0.4333886660629369 Val Acc: 0.8792656292656292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229/229 [01:40<00:00,  2.27it/s]\n",
      "100%|██████████| 58/58 [00:07<00:00,  8.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Train Loss: 0.2091996848522198 Val Loss: 0.4426247705096532 Val Acc: 0.8793338793338793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229/229 [01:48<00:00,  2.11it/s]\n",
      "100%|██████████| 58/58 [00:09<00:00,  5.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Train Loss: 0.19314123682805515 Val Loss: 0.45493219819285 Val Acc: 0.8788561288561288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model2 = CNN(pool=False)\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=1e-3)\n",
    "best_acc = -np.inf\n",
    "epochs = 10\n",
    "for e in range(1, epochs + 1):\n",
    "    train_loss = train(model2, train_loader, optimizer)\n",
    "    val_loss, val_acc = eval(model2, val_loader)\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_model2 = model2\n",
    "    print(f\"Epoch: {e} Train Loss: {train_loss} Val Loss: {val_loss} Val Acc: {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "13gIhnYVAbgQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102/102 [00:15<00:00,  6.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8651659496004918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_, test_acc = eval(best_model2, test_loader)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pooling in CNNs affects how they work in a few key ways:\n",
    "\n",
    "**Size of the Feature Maps:**\n",
    "- **With Pooling:** It makes the feature maps smaller, so there are fewer things to calculate and fewer parameters. This helps stop the model from fitting too closely to the training data.\n",
    "- **Without Pooling:** The feature maps stay bigger, leading to more calculations and more parameters.\n",
    "\n",
    "**Learning Features:**\n",
    "- **With Pooling:** Pooling helps the model learn more general features. It sort of summarizes what's in different parts of the input, making the model less affected by small shifts in where features are.\n",
    "- **Without Pooling:** The model keeps more detailed information about where things are in the input. This can be good or bad – it's more detailed, but it might make the model too sensitive to small changes.\n",
    "\n",
    "**Generalizing and Overfitting:**\n",
    "- **With Pooling:** Generally, pooling makes the model better at working with new, unseen data. It becomes less bothered by small changes and noise in the input.\n",
    "- **Without Pooling:** The model might learn the training data too well (overfit) because it has more parameters and keeps more detailed information.\n",
    "\n",
    "**Accuracy and Computational Costs:**\n",
    "- Disabling max pooling impacts both how accurate the model is and how much computing power it needs.\n",
    "- A model with max pooling got a higher accuracy (89.15%) than one without it (87.8%). This shows that pooling helps the model generalize better and pick out stronger features, which is key for doing well with new data.\n",
    "- Max pooling also means less computation – fewer parameters and less complexity. So, the model with pooling was faster and needed less memory. But, you do lose some detailed information about where things are, which might matter for some tasks.\n",
    "\n",
    "In summary, using pooling in a CNN is a balance. It can make the model faster, less memory-hungry, and more accurate for general tasks, but you might lose some detailed spatial information that's important for certain kinds of jobs."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
